From d6d15701d00695f9992d839f2c1ca3cb956e6d05 Mon Sep 17 00:00:00 2001
From: John Cox <jc@kynesim.co.uk>
Date: Wed, 1 Jun 2022 17:49:26 +0000
Subject: [PATCH 116/151] rpi_sand: Add SAND30->NV12 asm for Armv7 & Armv8

Also reworks the previous Armv8 SAND30->Y16 function in a slightly more
efficient way that makes it look more like the Armv7 version.
---
 libavutil/aarch64/rpi_sand_neon.S | 549 ++++++++++++++++++------------
 libavutil/aarch64/rpi_sand_neon.h |   4 +
 libavutil/arm/rpi_sand_neon.S     | 167 +++++++++
 libavutil/arm/rpi_sand_neon.h     |  11 +
 libavutil/rpi_sand_fns.c          |   2 +-
 5 files changed, 510 insertions(+), 223 deletions(-)

diff --git a/libavutil/aarch64/rpi_sand_neon.S b/libavutil/aarch64/rpi_sand_neon.S
index cdcf71ee67..2f07d9674c 100644
--- a/libavutil/aarch64/rpi_sand_neon.S
+++ b/libavutil/aarch64/rpi_sand_neon.S
@@ -248,228 +248,6 @@ incomplete_block_loop_end_c8:
     ret
 endfunc
 
-//void ff_rpi_sand30_lines_to_planar_y16(
-//  uint8_t * dest,             // [x0]
-//  unsigned int dst_stride,    // [w1] -> assumed to be equal to _w
-//  const uint8_t * src,        // [x2]
-//  unsigned int src_stride1,   // [w3] -> 128
-//  unsigned int src_stride2,   // [w4]
-//  unsigned int _x,            // [w5]
-//  unsigned int y,             // [w6]
-//  unsigned int _w,            // [w7]
-//  unsigned int h);            // [sp, #0]
-
-function ff_rpi_sand30_lines_to_planar_y16, export=1
-    stp x19, x20, [sp, #-48]!
-    stp x21, x22, [sp, #16]
-    stp x23, x24, [sp, #32]
-
-    // w6 = argument h
-    ldr w6, [sp, #48]
-
-    // slice_inc = ((stride2 - 1) * stride1)
-    mov w5, w4
-    sub w5, w5, #1
-    lsl w5, w5, #7
-
-    // total number of bytes per row = (width / 3) * 4
-    mov w8, w7
-    mov w9, #3
-    udiv w8, w8, w9
-    lsl w8, w8, #2
-
-    // number of full 128 byte blocks to be processed
-    mov w9, #96
-    udiv w9, w7, w9 // = (width * 4) / (3*128) = width/96
-
-    // w10 = number of full integers to process (4 bytes)
-    // w11 = remaning zero to two 10bit values still to copy over
-    mov w12, #96
-    mul w12, w9, w12
-    sub w12, w7, w12  // width - blocks*96 = remaining points per row
-    mov w11, #3
-    udiv w10, w12, w11 // full integers to process = w12 / 3 
-    mul w11, w10, w11  // #integers *3
-    sub w11, w12, w11  // remaining 0-2 points = remaining points - integers*3
-
-    // increase w9 by one if w10+w11 is not zero, and decrease the row count by one
-    // this is to efficiently copy incomplete blocks at the end of the rows
-    // the last row is handled explicitly to avoid writing out of bounds
-    add w22, w10, w11
-    cmp w22, #0
-    cset w22, ne // 1 iff w10+w11 not zero, 0 otherwise
-    add w9, w9, w22
-    sub w6, w6, #1
-
-    // store the number of bytes in w20 which we copy too much for every row
-    // when the width of the frame is not a multiple of 96 (128bytes storing 96 10bit values)
-    mov w20, #96*2
-    mul w20, w20, w9
-    sub w20, w1, w20
-
-    mov w23, #0 // flag to check whether the last line had already been processed
-    
-    // bitmask to clear the uppper 6bits of the result values
-    mov x19, #0x03ff03ff03ff03ff
-    dup v22.2d, x19
-
-    // row counter = 0
-    eor w12, w12, w12
-row_loop_y16:
-    cmp w12, w6               // jump to row_loop_y16_fin if we processed all rows
-    bge row_loop_y16_fin
-
-    mov x13, x2               // row src
-    eor w14, w14, w14         // full block counter
-block_loop_y16:
-    cmp w14, w9
-    bge block_loop_y16_fin
-
-    // load 64 bytes
-    ld1 { v0.4s,  v1.4s, v2.4s, v3.4s }, [x13], #64
-   
-    // process v0 and v1
-    xtn v16.4h, v0.4s
-    ushr v0.4s, v0.4s, #10
-    xtn v17.4h, v0.4s
-    ushr v0.4s, v0.4s, #10
-    xtn v18.4h, v0.4s
-   
-    xtn2 v16.8h, v1.4s
-    and v16.16b, v16.16b, v22.16b
-    ushr v1.4s, v1.4s, #10
-    xtn2 v17.8h, v1.4s
-    and v17.16b, v17.16b, v22.16b
-    ushr v1.4s, v1.4s, #10
-    xtn2 v18.8h, v1.4s
-    and v18.16b, v18.16b, v22.16b
-
-    st3 { v16.8h, v17.8h, v18.8h }, [x0], #48
-
-    // process v2 and v3
-    xtn v23.4h, v2.4s
-    ushr v2.4s, v2.4s, #10
-    xtn v24.4h, v2.4s
-    ushr v2.4s, v2.4s, #10
-    xtn v25.4h, v2.4s
-    
-    xtn2 v23.8h, v3.4s
-    and v23.16b, v23.16b, v22.16b
-    ushr v3.4s, v3.4s, #10
-    xtn2 v24.8h, v3.4s
-    and v24.16b, v24.16b, v22.16b
-    ushr v3.4s, v3.4s, #10
-    xtn2 v25.8h, v3.4s
-    and v25.16b, v25.16b, v22.16b
-
-    st3 { v23.8h, v24.8h, v25.8h }, [x0], #48
-
-    // load the second half of the block -> 64 bytes into registers v4-v7
-    ld1 { v4.4s,  v5.4s,  v6.4s,  v7.4s }, [x13], #64
-    
-    // process v4 and v5
-    xtn v16.4h, v4.4s
-    ushr v4.4s, v4.4s, #10
-    xtn v17.4h, v4.4s
-    ushr v4.4s, v4.4s, #10
-    xtn v18.4h, v4.4s
-   
-    xtn2 v16.8h, v5.4s 
-    and v16.16b, v16.16b, v22.16b
-    ushr v5.4s, v5.4s, #10
-    xtn2 v17.8h, v5.4s
-    and v17.16b, v17.16b, v22.16b
-    ushr v5.4s, v5.4s, #10
-    xtn2 v18.8h, v5.4s
-    and v18.16b, v18.16b, v22.16b
-
-    st3 { v16.8h, v17.8h, v18.8h }, [x0], #48
-
-    // v6 and v7
-    xtn v23.4h, v6.4s
-    ushr v6.4s, v6.4s, #10
-    xtn v24.4h, v6.4s
-    ushr v6.4s, v6.4s, #10
-    xtn v25.4h, v6.4s
-   
-    xtn2 v23.8h, v7.4s 
-    and v23.16b, v23.16b, v22.16b
-    ushr v7.4s, v7.4s, #10
-    xtn2 v24.8h, v7.4s
-    and v24.16b, v24.16b, v22.16b
-    ushr v7.4s, v7.4s, #10
-    xtn2 v25.8h, v7.4s
-    and v25.16b, v25.16b, v22.16b
-
-    st3 { v23.8h, v24.8h, v25.8h }, [x0], #48
- 
-    add x13, x13, x5          // row src += slice_inc
-    add w14, w14, #1
-    b block_loop_y16
-block_loop_y16_fin:
-
-    
-
-
-    add x2, x2, #128          // src += stride1 (start of the next row)
-    add x0, x0, w20, sxtw     // subtract the bytes we copied too much from dst
-    add w12, w12, #1
-    b row_loop_y16
-row_loop_y16_fin:
-
-    // check whether we have incomplete blocks at the end of every row
-    // in that case decrease row block count by one
-    // change height back to it's original value (meaning increase it by 1)
-    // and jump back to another iteration of row_loop_y16
-
-    cmp w23, #1
-    beq row_loop_y16_fin2 // don't continue here if we already processed the last row
-    add w6, w6, #1    // increase height to the original value
-    sub w9, w9, w22   // block count - 1 or 0, depending on the remaining bytes count
-    mov w23, #1
-    b row_loop_y16
-row_loop_y16_fin2:
-
-    sub x0, x0, w20, sxtw // with the last row we didn't actually move the dst ptr to far ahead, therefore readd the diference
-
-    // now we've got to handle the last block in the last row
-    eor w12, w12, w12 // w12 = 0 = counter
-integer_loop_y16:
-    cmp w12, w10
-    bge integer_loop_y16_fin
-    ldr w14, [x13], #4
-    and w15, w14, #0x3ff
-    strh w15, [x0], #2
-    lsr w14, w14, #10
-    and w15, w14, #0x3ff
-    strh w15, [x0], #2
-    lsr w14, w14, #10
-    and w15, w14, #0x3ff
-    strh w15, [x0], #2
-    add w12, w12, #1
-    b integer_loop_y16
-integer_loop_y16_fin:
-
-final_values_y16:
-    // remaining point count = w11
-    ldr w14, [x13], #4
-    cmp w11, #0
-    beq final_values_y16_fin
-    and w15, w14, #0x3ff
-    strh w15, [x0], #2
-    cmp w11, #1
-    beq final_values_y16_fin
-    lsr w14, w14, #10
-    and w15, w14, #0x3ff
-    strh w15, [x0], #2
-final_values_y16_fin:
-
-    ldp x23, x24, [sp, #32]
-    ldp x21, x22, [sp, #16]
-    ldp x19, x20, [sp], #48
-    ret
-endfunc
-
 //void ff_rpi_sand30_lines_to_planar_c16(
 //  uint8_t * dst_u,            // [x0]
 //  unsigned int dst_stride_u,  // [w1] == _w*2
@@ -674,3 +452,330 @@ endfunc
 //  unsigned int _w,
 //  unsigned int h);
 
+// void ff_rpi_sand30_lines_to_planar_y8(
+//   uint8_t * dest,            : x0
+//   unsigned int dst_stride,   : w1
+//   const uint8_t * src,       : x2
+//   unsigned int src_stride1,  : w3, always 128
+//   unsigned int src_stride2,  : w4
+//   unsigned int _x,           : w5
+//   unsigned int y,            : w6
+//   unsigned int _w,           : w7
+//   unsigned int h);           : [sp, #0]
+//
+// Assumes that we are starting on a stripe boundary and that overreading
+// within the stripe is OK. However it does respect the dest size for wri
+
+function ff_rpi_sand30_lines_to_planar_y16, export=1
+                lsl             w4,  w4,  #7
+                sub             w4,  w4,  #64
+                sub             w1,  w1,  w7, lsl #1
+                uxtw            x6,  w6
+                add             x8,  x2,  x6, lsl #7
+                ldr             w6,  [sp, #0]
+
+10:
+                mov             x2,  x8
+                mov             w5,  w7
+1:
+                ld1             {v0.4s, v1.4s, v2.4s, v3.4s}, [x2], #64
+                ld1             {v4.4s, v5.4s, v6.4s, v7.4s}, [x2], x4
+
+                subs            w5,  w5,  #96
+
+                // v0, v1
+
+                shrn            v18.4h,  v0.4s,   #14
+                xtn             v16.4h,  v0.4s
+                shrn            v17.4h,  v0.4s,   #10
+
+                shrn2           v18.8h,  v1.4s,   #14
+                xtn2            v16.8h,  v1.4s
+                shrn2           v17.8h,  v1.4s,   #10
+
+                ushr            v18.8h,  v18.8h,  #6
+                bic             v16.8h,  #0xfc,   lsl #8
+                bic             v17.8h,  #0xfc,   lsl #8
+
+                // v2, v3
+
+                shrn            v21.4h,  v2.4s,   #14
+                xtn             v19.4h,  v2.4s
+                shrn            v20.4h,  v2.4s,   #10
+
+                shrn2           v21.8h,  v3.4s,   #14
+                xtn2            v19.8h,  v3.4s
+                shrn2           v20.8h,  v3.4s,   #10
+
+                ushr            v21.8h,  v21.8h,  #6
+                bic             v19.8h,  #0xfc,   lsl #8
+                bic             v20.8h,  #0xfc,   lsl #8
+
+                // v4, v5
+
+                shrn            v24.4h,  v4.4s,   #14
+                xtn             v22.4h,  v4.4s
+                shrn            v23.4h,  v4.4s,   #10
+
+                shrn2           v24.8h,  v5.4s,   #14
+                xtn2            v22.8h,  v5.4s
+                shrn2           v23.8h,  v5.4s,   #10
+
+                ushr            v24.8h,  v24.8h,  #6
+                bic             v22.8h,  #0xfc,   lsl #8
+                bic             v23.8h,  #0xfc,   lsl #8
+
+                // v6, v7
+
+                shrn            v27.4h,  v6.4s,   #14
+                xtn             v25.4h,  v6.4s
+                shrn            v26.4h,  v6.4s,   #10
+
+                shrn2           v27.8h,  v7.4s,   #14
+                xtn2            v25.8h,  v7.4s
+                shrn2           v26.8h,  v7.4s,   #10
+
+                ushr            v27.8h,  v27.8h,  #6
+                bic             v25.8h,  #0xfc,   lsl #8
+                bic             v26.8h,  #0xfc,   lsl #8
+
+                blt             2f
+
+                st3             {v16.8h, v17.8h, v18.8h}, [x0], #48
+                st3             {v19.8h, v20.8h, v21.8h}, [x0], #48
+                st3             {v22.8h, v23.8h, v24.8h}, [x0], #48
+                st3             {v25.8h, v26.8h, v27.8h}, [x0], #48
+
+                bne             1b
+
+11:
+                subs            w6,  w6,  #1
+                add             x0,  x0,  w1,  uxtw
+                add             x8,  x8,  #128
+                bne             10b
+
+                ret
+
+// Partial final write
+2:
+                cmp             w5,  #48-96
+                blt             1f
+                st3             {v16.8h, v17.8h, v18.8h}, [x0], #48
+                st3             {v19.8h, v20.8h, v21.8h}, [x0], #48
+                beq             11b
+                mov             v16.16b, v22.16b
+                mov             v17.16b, v23.16b
+                sub             w5,  w5,  #48
+                mov             v18.16b, v24.16b
+                mov             v19.16b, v25.16b
+                mov             v20.16b, v26.16b
+                mov             v21.16b, v27.16b
+1:
+                cmp             w5,  #24-96
+                blt             1f
+                st3             {v16.8h, v17.8h, v18.8h}, [x0], #48
+                beq             11b
+                mov             v16.16b, v19.16b
+                mov             v17.16b, v20.16b
+                sub             w5,  w5,  #24
+                mov             v18.16b, v21.16b
+1:
+                cmp             w5,  #12-96
+                blt             1f
+                st3             {v16.4h, v17.4h, v18.4h}, [x0], #24
+                beq             11b
+                mov             v16.2d[0], v16.2d[1]
+                sub             w5,  w5,  #12
+                mov             v17.2d[0], v17.2d[1]
+                mov             v18.2d[0], v18.2d[1]
+1:
+                cmp             w5,  #6-96
+                blt             1f
+                st3             {v16.h, v17.h, v18.h}[0], [x0], #6
+                st3             {v16.h, v17.h, v18.h}[1], [x0], #6
+                beq             11b
+                mov             v16.2s[0], v16.2s[1]
+                sub             w5,  w5,  #6
+                mov             v17.2s[0], v17.2s[1]
+                mov             v18.2s[0], v18.2s[1]
+1:
+                cmp             w5,  #3-96
+                blt             1f
+                st3             {v16.h, v17.h, v18.h}[0], [x0], #6
+                beq             11b
+                mov             v16.4h[0], v16.4h[1]
+                sub             w5,  w5,  #3
+                mov             v17.4h[0], v17.4h[1]
+1:
+                cmp             w5,  #2-96
+                blt             1f
+                st2             {v16.h, v17.h}[0], [x0], #4
+                b               11b
+1:
+                st1             {v16.h}[0], [x0], #2
+                b               11b
+
+endfunc
+
+// void ff_rpi_sand30_lines_to_planar_y8(
+//   uint8_t * dest,            : x0
+//   unsigned int dst_stride,   : w1
+//   const uint8_t * src,       : x2
+//   unsigned int src_stride1,  : w3, always 128
+//   unsigned int src_stride2,  : w4
+//   unsigned int _x,           : w5
+//   unsigned int y,            : w6
+//   unsigned int _w,           : w7
+//   unsigned int h);           : [sp, #0]
+//
+// Assumes that we are starting on a stripe boundary and that overreading
+// within the stripe is OK. However it does respect the dest size for wri
+
+function ff_rpi_sand30_lines_to_planar_y8, export=1
+                lsl             w4,  w4,  #7
+                sub             w4,  w4,  #64
+                sub             w1,  w1,  w7
+                uxtw            x6,  w6
+                add             x8,  x2,  x6, lsl #7
+                ldr             w6,  [sp, #0]
+
+10:
+                mov             x2,  x8
+                mov             w5,  w7
+1:
+                ld1             {v0.4s, v1.4s, v2.4s, v3.4s}, [x2], #64
+                ld1             {v4.4s, v5.4s, v6.4s, v7.4s}, [x2], x4
+
+                subs            w5,  w5,  #96
+
+                // v0, v1
+
+                shrn            v18.4h,  v0.4s,   #16
+                xtn             v16.4h,  v0.4s
+                shrn            v17.4h,  v0.4s,   #12
+
+                shrn2           v18.8h,  v1.4s,   #16
+                xtn2            v16.8h,  v1.4s
+                shrn2           v17.8h,  v1.4s,   #12
+
+                shrn            v18.8b,  v18.8h,  #6
+                shrn            v16.8b,  v16.8h,  #2
+                xtn             v17.8b,  v17.8h
+
+                // v2, v3
+
+                shrn            v21.4h,  v2.4s,   #16
+                xtn             v19.4h,  v2.4s
+                shrn            v20.4h,  v2.4s,   #12
+
+                shrn2           v21.8h,  v3.4s,   #16
+                xtn2            v19.8h,  v3.4s
+                shrn2           v20.8h,  v3.4s,   #12
+
+                shrn2           v18.16b, v21.8h,  #6
+                shrn2           v16.16b, v19.8h,  #2
+                xtn2            v17.16b, v20.8h
+
+                // v4, v5
+
+                shrn            v24.4h,  v4.4s,   #16
+                xtn             v22.4h,  v4.4s
+                shrn            v23.4h,  v4.4s,   #12
+
+                shrn2           v24.8h,  v5.4s,   #16
+                xtn2            v22.8h,  v5.4s
+                shrn2           v23.8h,  v5.4s,   #12
+
+                shrn            v21.8b,  v24.8h,  #6
+                shrn            v19.8b,  v22.8h,  #2
+                xtn             v20.8b,  v23.8h
+
+                // v6, v7
+
+                shrn            v27.4h,  v6.4s,   #16
+                xtn             v25.4h,  v6.4s
+                shrn            v26.4h,  v6.4s,   #12
+
+                shrn2           v27.8h,  v7.4s,   #16
+                xtn2            v25.8h,  v7.4s
+                shrn2           v26.8h,  v7.4s,   #12
+
+                shrn2           v21.16b, v27.8h,  #6
+                shrn2           v19.16b, v25.8h,  #2
+                xtn2            v20.16b, v26.8h
+
+                blt             2f
+
+                st3             {v16.16b, v17.16b, v18.16b}, [x0], #48
+                st3             {v19.16b, v20.16b, v21.16b}, [x0], #48
+
+                bne             1b
+
+11:
+                subs            w6,  w6,  #1
+                add             x0,  x0,  w1,  uxtw
+                add             x8,  x8,  #128
+                bne             10b
+
+                ret
+
+// Partial final write
+2:
+                cmp             w5,  #48-96
+                blt             1f
+                st3             {v16.16b, v17.16b, v18.16b}, [x0], #48
+                beq             11b
+                mov             v16.16b, v22.16b
+                mov             v17.16b, v23.16b
+                sub             w5,  w5,  #48
+                mov             v18.16b, v24.16b
+1:
+                cmp             w5,  #24-96
+                blt             1f
+                st3             {v16.8b, v17.8b, v18.8b}, [x0], #24
+                beq             11b
+                mov             v16.2d[0], v16.2d[1]
+                sub             w5,  w5,  #24
+                mov             v17.2d[0], v17.2d[1]
+                mov             v18.2d[0], v18.2d[1]
+1:
+                cmp             w5,  #12-96
+                blt             1f
+                st3             {v16.b, v17.b, v18.b}[0], [x0], #3
+                st3             {v16.b, v17.b, v18.b}[1], [x0], #3
+                st3             {v16.b, v17.b, v18.b}[2], [x0], #3
+                st3             {v16.b, v17.b, v18.b}[3], [x0], #3
+                beq             11b
+                mov             v16.2s[0], v16.2s[1]
+                sub             w5,  w5,  #12
+                mov             v17.2s[0], v17.2s[1]
+                mov             v18.2s[0], v18.2s[1]
+1:
+                cmp             w5,  #6-96
+                blt             1f
+                st3             {v16.b, v17.b, v18.b}[0], [x0], #3
+                st3             {v16.b, v17.b, v18.b}[1], [x0], #3
+                beq             11b
+                mov             v16.4h[0], v16.4h[1]
+                sub             w5,  w5,  #6
+                mov             v17.4h[0], v17.4h[1]
+                mov             v18.4h[0], v18.4h[1]
+1:
+                cmp             w5,  #3-96
+                blt             1f
+                st3             {v16.b, v17.b, v18.b}[0], [x0], #3
+                beq             11b
+                mov             v16.8b[0], v16.8b[1]
+                sub             w5,  w5,  #3
+                mov             v17.8b[0], v17.8b[1]
+1:
+                cmp             w5,  #2-96
+                blt             1f
+                st2             {v16.b, v17.b}[0], [x0], #2
+                b               11b
+1:
+                st1             {v16.b}[0], [x0], #1
+                b               11b
+
+endfunc
+
diff --git a/libavutil/aarch64/rpi_sand_neon.h b/libavutil/aarch64/rpi_sand_neon.h
index b3aa481ea4..2a56135bc3 100644
--- a/libavutil/aarch64/rpi_sand_neon.h
+++ b/libavutil/aarch64/rpi_sand_neon.h
@@ -49,6 +49,10 @@ void ff_rpi_sand30_lines_to_planar_c16(uint8_t * dst_u, unsigned int dst_stride_
   uint8_t * dst_v, unsigned int dst_stride_v, const uint8_t * src, unsigned int stride1,
   unsigned int stride2, unsigned int _x, unsigned int y, unsigned int _w, unsigned int h);
 
+void ff_rpi_sand30_lines_to_planar_y8(uint8_t * dest, unsigned int dst_stride,
+  const uint8_t * src, unsigned int src_stride1, unsigned int src_stride2,
+  unsigned int _x, unsigned int y, unsigned int _w, unsigned int h);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/libavutil/arm/rpi_sand_neon.S b/libavutil/arm/rpi_sand_neon.S
index 80890fe985..a07111c8fc 100644
--- a/libavutil/arm/rpi_sand_neon.S
+++ b/libavutil/arm/rpi_sand_neon.S
@@ -765,4 +765,171 @@ function ff_rpi_sand30_lines_to_planar_p010, export=1
 endfunc
 
 
+@ void ff_rpi_sand30_lines_to_planar_y8(
+@   uint8_t * dest,             // [r0]
+@   unsigned int dst_stride,    // [r1]
+@   const uint8_t * src,        // [r2]
+@   unsigned int src_stride1,   // [r3]      Ignored - assumed 128
+@   unsigned int src_stride2,   // [sp, #0]  -> r3
+@   unsigned int _x,            // [sp, #4]  Ignored - 0
+@   unsigned int y,             // [sp, #8]  (r7 in prefix)
+@   unsigned int _w,            // [sp, #12] -> r6 (cur r5)
+@   unsigned int h);            // [sp, #16] -> r7
+@
+@ Assumes that we are starting on a stripe boundary and that overreading
+@ within the stripe is OK. However it does respect the dest size for wri
+
+function ff_rpi_sand30_lines_to_planar_y8, export=1
+                push            {r4-r8, lr}     @ +24
+                ldr             r3,  [sp, #24]
+                ldr             r6,  [sp, #36]
+                ldr             r7,  [sp, #32]  @ y
+                mov             r12, #48
+                lsl             r3,  #7
+                sub             r1,  r1,  r6
+                add             r8,  r2,  r7,  lsl #7
+                ldr             r7,  [sp, #40]
+
+10:
+                mov             r2,  r8
+                add             r4,  r0,  #24
+                mov             r5,  r6
+1:
+                vldm            r2,  {q8-q15}
+
+                subs            r5,  #96
+
+                vmovn.u32       d0,  q8
+                vshrn.u32       d2,  q8,  #12
+                vshrn.u32       d4,  q8,  #16    @ Cannot vshrn.u32 #20!
+
+                add             r2,  r3
+
+                vmovn.u32       d1,  q9
+                vshrn.u32       d3,  q9,  #12
+                vshrn.u32       d5,  q9,  #16
+
+                pld             [r2, #0]
+
+                vshrn.u16       d0,  q0,  #2
+                vmovn.u16       d1,  q1
+                vshrn.u16       d2,  q2,  #6
+
+                vmovn.u32       d16, q10
+                vshrn.u32       d18, q10, #12
+                vshrn.u32       d20, q10, #16
+
+                vmovn.u32       d17, q11
+                vshrn.u32       d19, q11, #12
+                vshrn.u32       d21, q11, #16
+
+                pld             [r2, #64]
+
+                vshrn.u16       d4,  q8,  #2
+                vmovn.u16       d5,  q9
+                vshrn.u16       d6,  q10, #6
+
+                vmovn.u32       d16, q12
+                vshrn.u32       d18, q12, #12
+                vshrn.u32       d20, q12, #16
+
+                vmovn.u32       d17, q13
+                vshrn.u32       d19, q13, #12
+                vshrn.u32       d21, q13, #16
+
+                vshrn.u16       d16, q8,  #2
+                vmovn.u16       d17, q9
+                vshrn.u16       d18, q10, #6
+
+                vmovn.u32       d20, q14
+                vshrn.u32       d22, q14, #12
+                vshrn.u32       d24, q14, #16
+
+                vmovn.u32       d21, q15
+                vshrn.u32       d23, q15, #12
+                vshrn.u32       d25, q15, #16
+
+                vshrn.u16       d20, q10, #2
+                vmovn.u16       d21, q11
+                vshrn.u16       d22, q12, #6
+
+                blt             2f
+
+                vst3.8          {d0,  d1,  d2},  [r0], r12
+                vst3.8          {d4,  d5,  d6},  [r4], r12
+                vst3.8          {d16, d17, d18}, [r0], r12
+                vst3.8          {d20, d21, d22}, [r4], r12
+
+                bne             1b
+
+11:
+                subs            r7,  #1
+                add             r0,  r1
+                add             r8,  #128
+                bne             10b
+
+                pop             {r4-r8, pc}
+
+@ Partial final write
+2:
+                cmp             r5,  #48-96
+                blt             1f
+                vst3.8          {d0,  d1,  d2},  [r0], r12
+                vst3.8          {d4,  d5,  d6},  [r4], r12
+                beq             11b
+                vmov            q0,  q8
+                vmov            q2,  q10
+                sub             r5,  #48
+                vmov            d2,  d18
+                vmov            d6,  d22
+1:
+                cmp             r5,  #24-96
+                blt             1f
+                vst3.8          {d0,  d1,  d2},  [r0]!
+                beq             11b
+                vmov            q0,  q2
+                sub             r5,  #24
+                vmov            d2,  d6
+1:
+                cmp             r5,  #12-96
+                blt             1f
+                vst3.8          {d0[0], d1[0], d2[0]}, [r0]!
+                vst3.8          {d0[1], d1[1], d2[1]}, [r0]!
+                vst3.8          {d0[2], d1[2], d2[2]}, [r0]!
+                vst3.8          {d0[3], d1[3], d2[3]}, [r0]!
+                beq             11b
+                vmov            s0,  s1
+                sub             r5,  #12
+                vmov            s2,  s3
+                vmov            s4,  s5
+1:
+                cmp             r5,  #6-96
+                blt             1f
+                vst3.8          {d0[0], d1[0], d2[0]}, [r0]!
+                vst3.8          {d0[1], d1[1], d2[1]}, [r0]!
+                add             r0,  #12
+                beq             11b
+                vshr.u32        d0,  #16
+                sub             r5,  #6
+                vshr.u32        d1,  #16
+                vshr.u32        d2,  #16
+1:
+                cmp             r5, #3-96
+                blt             1f
+                vst3.8          {d0[0], d1[0], d2[0]}, [r0]!
+                beq             11b
+                sub             r5, #3
+                vshr.u32        d0, #8
+                vshr.u32        d1, #8
+1:
+                cmp             r5, #2-96
+                blt             1f
+                vst2.8          {d0[0], d1[0]}, [r0]!
+                b               11b
+1:
+                vst1.8          {d0[0]}, [r0]!
+                b               11b
+
+endfunc
+
 
diff --git a/libavutil/arm/rpi_sand_neon.h b/libavutil/arm/rpi_sand_neon.h
index 447f367bea..d457c10870 100644
--- a/libavutil/arm/rpi_sand_neon.h
+++ b/libavutil/arm/rpi_sand_neon.h
@@ -95,5 +95,16 @@ void ff_rpi_sand30_lines_to_planar_p010(
   unsigned int _w,            // [sp, #12] -> r6 (cur r5)
   unsigned int h);            // [sp, #16] -> r7
 
+void ff_rpi_sand30_lines_to_planar_y8(
+  uint8_t * dest,             // [r0]
+  unsigned int dst_stride,    // [r1]
+  const uint8_t * src,        // [r2]
+  unsigned int src_stride1,   // [r3]      Ignored - assumed 128
+  unsigned int src_stride2,   // [sp, #0]  -> r3
+  unsigned int _x,            // [sp, #4]  Ignored - 0
+  unsigned int y,             // [sp, #8]  (r7 in prefix)
+  unsigned int _w,            // [sp, #12] -> r6 (cur r5)
+  unsigned int h);            // [sp, #16] -> r7
+
 #endif // AVUTIL_ARM_SAND_NEON_H
 
diff --git a/libavutil/rpi_sand_fns.c b/libavutil/rpi_sand_fns.c
index 256c3d532f..b6071e2928 100644
--- a/libavutil/rpi_sand_fns.c
+++ b/libavutil/rpi_sand_fns.c
@@ -247,7 +247,7 @@ void av_rpi_sand30_to_planar_y8(uint8_t * dst, const unsigned int dst_stride,
     const uint8_t * p0 = src + (x0 & mask) + y * stride1 + (x0 & ~mask) * stride2;
     const unsigned int slice_inc = ((stride2 - 1) * stride1) >> 2;  // RHS of a stripe to LHS of next in words
 
-#if HAVE_SAND_ASM && 0
+#if HAVE_SAND_ASM
     if (_x == 0) {
         ff_rpi_sand30_lines_to_planar_y8(dst, dst_stride, src, stride1, stride2, _x, y, _w, h);
         return;
-- 
2.38.1

